{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6823a1f6-7d38-486d-8348-b342a1345c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"ABS_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a774d5f-9108-4af7-9a9d-ed3c0b3d2684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Sub Domain</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Link</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Customer Rating</th>\n",
       "      <th>Customer Images</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Finish</th>\n",
       "      <th>Formulation</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Complaint Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>Lakme 9 to 5 Weightless Mousse Lip &amp; Cheek Col...</td>\n",
       "      <td>https://www.amazon.in/Lakme-Weightless-Mousse-...</td>\n",
       "      <td>The shade of the product recieved is different...</td>\n",
       "      <td>03 Sep 2022</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://m.media-amazon.com/images/W/MEDIAX_7...</td>\n",
       "      <td>Coffee Lite</td>\n",
       "      <td>Matte</td>\n",
       "      <td>Liquid</td>\n",
       "      <td>Full Coverage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>Lakme 9 to 5 Weightless Mousse Lip &amp; Cheek Col...</td>\n",
       "      <td>https://www.amazon.in/Lakme-Weightless-Mousse-...</td>\n",
       "      <td>MISTAKE... WORTH LESS N USELESS DRY INSIDE I g...</td>\n",
       "      <td>15 Aug 2022</td>\n",
       "      <td>Heroji Rao Khatkar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://m.media-amazon.com/images/W/MEDIAX_7...</td>\n",
       "      <td>Coffee Lite</td>\n",
       "      <td>Matte</td>\n",
       "      <td>Liquid</td>\n",
       "      <td>Full Coverage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>Lakme 9 to 5 Weightless Mousse Lip &amp; Cheek Col...</td>\n",
       "      <td>https://www.amazon.in/Lakme-Weightless-Mousse-...</td>\n",
       "      <td>The lip shade is totally dried. I have previou...</td>\n",
       "      <td>23 Jan 2021</td>\n",
       "      <td>Swathi Anveshi</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://images-na.ssl-images-amazon.com/imag...</td>\n",
       "      <td>Coffee Lite</td>\n",
       "      <td>Matte</td>\n",
       "      <td>Liquid</td>\n",
       "      <td>Full Coverage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>Lakme 9 to 5 Weightless Mousse Lip &amp; Cheek Col...</td>\n",
       "      <td>https://www.amazon.in/Lakme-Weightless-Mousse-...</td>\n",
       "      <td>The media could not be loaded. Its nothing but...</td>\n",
       "      <td>13 Aug 2022</td>\n",
       "      <td>Pravin Kumar Pandey</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coffee Lite</td>\n",
       "      <td>Matte</td>\n",
       "      <td>Liquid</td>\n",
       "      <td>Full Coverage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>Lakme 9 to 5 Weightless Mousse Lip &amp; Cheek Col...</td>\n",
       "      <td>https://www.amazon.in/Lakme-Weightless-Mousse-...</td>\n",
       "      <td>The media could not be loaded. Pointer broken ...</td>\n",
       "      <td>06 Sep 2022</td>\n",
       "      <td>Atithi Maske</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['https://m.media-amazon.com/images/W/MEDIAX_7...</td>\n",
       "      <td>Coffee Lite</td>\n",
       "      <td>Matte</td>\n",
       "      <td>Liquid</td>\n",
       "      <td>Full Coverage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Domain Sub Domain                                       Product Name  \\\n",
       "0  Lipstick      Lakme  Lakme 9 to 5 Weightless Mousse Lip & Cheek Col...   \n",
       "1  Lipstick      Lakme  Lakme 9 to 5 Weightless Mousse Lip & Cheek Col...   \n",
       "2  Lipstick      Lakme  Lakme 9 to 5 Weightless Mousse Lip & Cheek Col...   \n",
       "3  Lipstick      Lakme  Lakme 9 to 5 Weightless Mousse Lip & Cheek Col...   \n",
       "4  Lipstick      Lakme  Lakme 9 to 5 Weightless Mousse Lip & Cheek Col...   \n",
       "\n",
       "                                        Product Link  \\\n",
       "0  https://www.amazon.in/Lakme-Weightless-Mousse-...   \n",
       "1  https://www.amazon.in/Lakme-Weightless-Mousse-...   \n",
       "2  https://www.amazon.in/Lakme-Weightless-Mousse-...   \n",
       "3  https://www.amazon.in/Lakme-Weightless-Mousse-...   \n",
       "4  https://www.amazon.in/Lakme-Weightless-Mousse-...   \n",
       "\n",
       "                                              Review  Review Date  \\\n",
       "0  The shade of the product recieved is different...  03 Sep 2022   \n",
       "1  MISTAKE... WORTH LESS N USELESS DRY INSIDE I g...  15 Aug 2022   \n",
       "2  The lip shade is totally dried. I have previou...  23 Jan 2021   \n",
       "3  The media could not be loaded. Its nothing but...  13 Aug 2022   \n",
       "4  The media could not be loaded. Pointer broken ...  06 Sep 2022   \n",
       "\n",
       "         Customer Name  Customer Rating  \\\n",
       "0                    S              1.0   \n",
       "1   Heroji Rao Khatkar              1.0   \n",
       "2       Swathi Anveshi              1.0   \n",
       "3  Pravin Kumar Pandey              1.0   \n",
       "4         Atithi Maske              1.0   \n",
       "\n",
       "                                     Customer Images       Colour Finish  \\\n",
       "0  ['https://m.media-amazon.com/images/W/MEDIAX_7...  Coffee Lite  Matte   \n",
       "1  ['https://m.media-amazon.com/images/W/MEDIAX_7...  Coffee Lite  Matte   \n",
       "2  ['https://images-na.ssl-images-amazon.com/imag...  Coffee Lite  Matte   \n",
       "3                                                NaN  Coffee Lite  Matte   \n",
       "4  ['https://m.media-amazon.com/images/W/MEDIAX_7...  Coffee Lite  Matte   \n",
       "\n",
       "  Formulation       Coverage  Complaint Label  \n",
       "0      Liquid  Full Coverage                1  \n",
       "1      Liquid  Full Coverage                1  \n",
       "2      Liquid  Full Coverage                1  \n",
       "3      Liquid  Full Coverage                1  \n",
       "4      Liquid  Full Coverage                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025e3e00-dfb8-4841-b88a-1fe702a57fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Domain', 'Sub Domain', 'Product Name', 'Product Link', 'Review',\n",
       "       'Review Date', 'Customer Name', 'Customer Rating', 'Customer Images',\n",
       "       'Colour', 'Finish', 'Formulation', 'Coverage', 'Complaint Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d14f51a-f6e2-4e8e-9741-06aba7b0937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cf2477-4c9f-474b-a21d-8ea3914b1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group reviews by domain and subdomain\n",
    "grouped_reviews = df.groupby(['Domain', 'Sub Domain'])['Review'].apply(list)\n",
    "\n",
    "# Step 3: Aggregate aspects\n",
    "aggregated_aspects = df.groupby(['Domain', 'Sub Domain'])[['Colour', 'Finish', 'Formulation', 'Coverage']].agg(lambda x: list(set(x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c2531e7-e496-43bd-8708-b66e83c21457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Domain      Sub Domain         \n",
       "Foundation  Lakme                  [I recieved the used product as there was no s...\n",
       "            Loreal Paris           [The Item firstly received without seal and se...\n",
       "            M.A.C                  [nan, This is how the make up came out of the ...\n",
       "            Maybelline New York    [Refer the image right side original, left fou...\n",
       "            Nykaa Fashion          [Creamy n easy to apply. Easy to blend, Creamy...\n",
       "Lipstick    Lakme                  [The shade of the product recieved is differen...\n",
       "            Loreal Paris           [I’ve already reviewed this lipstick earlier (...\n",
       "            M.A.C                  [Yar itna bebkoof banate h ye log...i got diff...\n",
       "            Maybelline New York    [I like only packing I dislike the product qua...\n",
       "            Nykaa Fashion          [nan, The media could not be loaded., This pro...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b03125-1c42-47e4-b5e3-69403799ac75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Domain</th>\n",
       "      <th>Sub Domain</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Finish</th>\n",
       "      <th>Formulation</th>\n",
       "      <th>Coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>[Rose Ivory]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Mousse]</td>\n",
       "      <td>[Lightweight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>Loreal Paris</td>\n",
       "      <td>[Rose Vanilla]</td>\n",
       "      <td>[Natural]</td>\n",
       "      <td>[Liquid]</td>\n",
       "      <td>[Full Coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>M.A.C</td>\n",
       "      <td>[Neutral Cool]</td>\n",
       "      <td>[Natural]</td>\n",
       "      <td>[Powder]</td>\n",
       "      <td>[Full Coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>Maybelline New York</td>\n",
       "      <td>[Natural Beige]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Lotion]</td>\n",
       "      <td>[Medium Coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>Nykaa Fashion</td>\n",
       "      <td>[Toffee Chisel]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Cream, Stick]</td>\n",
       "      <td>[Buildable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Lakme</td>\n",
       "      <td>[Coffee Lite]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Liquid]</td>\n",
       "      <td>[Full Coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Loreal Paris</td>\n",
       "      <td>[Pinkish Nude]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Liquid]</td>\n",
       "      <td>[Lightweight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>M.A.C</td>\n",
       "      <td>[Russian red]</td>\n",
       "      <td>[Semi-Matte, Matte]</td>\n",
       "      <td>[Stick]</td>\n",
       "      <td>[Full Coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Maybelline New York</td>\n",
       "      <td>[Almond Pink]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Stick]</td>\n",
       "      <td>[Full Coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lipstick</td>\n",
       "      <td>Nykaa Fashion</td>\n",
       "      <td>[Let it Snooze]</td>\n",
       "      <td>[Matte]</td>\n",
       "      <td>[Stick]</td>\n",
       "      <td>[Lightweight]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Domain           Sub Domain           Colour               Finish  \\\n",
       "0  Foundation                Lakme     [Rose Ivory]              [Matte]   \n",
       "1  Foundation         Loreal Paris   [Rose Vanilla]            [Natural]   \n",
       "2  Foundation                M.A.C   [Neutral Cool]            [Natural]   \n",
       "3  Foundation  Maybelline New York  [Natural Beige]              [Matte]   \n",
       "4  Foundation        Nykaa Fashion  [Toffee Chisel]              [Matte]   \n",
       "5    Lipstick                Lakme    [Coffee Lite]              [Matte]   \n",
       "6    Lipstick         Loreal Paris   [Pinkish Nude]              [Matte]   \n",
       "7    Lipstick                M.A.C    [Russian red]  [Semi-Matte, Matte]   \n",
       "8    Lipstick  Maybelline New York    [Almond Pink]              [Matte]   \n",
       "9    Lipstick        Nykaa Fashion  [Let it Snooze]              [Matte]   \n",
       "\n",
       "      Formulation           Coverage  \n",
       "0        [Mousse]      [Lightweight]  \n",
       "1        [Liquid]    [Full Coverage]  \n",
       "2        [Powder]    [Full Coverage]  \n",
       "3        [Lotion]  [Medium Coverage]  \n",
       "4  [Cream, Stick]        [Buildable]  \n",
       "5        [Liquid]    [Full Coverage]  \n",
       "6        [Liquid]      [Lightweight]  \n",
       "7         [Stick]    [Full Coverage]  \n",
       "8         [Stick]    [Full Coverage]  \n",
       "9         [Stick]      [Lightweight]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138e1825-08a9-4e88-8728-288ea1449ce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m model_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/switch-base-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatrickvonplaten/xprophetnet-large-uncased-standalone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUCAIBox/mvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/pegasus-xsum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/pegasus-x-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muclanlp/plbart-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/prophetnet-large-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[0;32m----> 9\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#model = AutoModelForSeq2SeqLM.from_pretrained(\"model_name\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-cnn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/fakereview/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:791\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    792\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             )\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    799\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BigBirdPegasusForConditionalGeneration, BlenderbotForConditionalGeneration, LEDForConditionalGeneration, T5ForConditionalGeneration, SwitchTransformersForConditionalGeneration, XLMProphetNetForConditionalGeneration, MvpForConditionalGeneration, PegasusForConditionalGeneration, PegasusXForConditionalGeneration, PLBartForConditionalGeneration, ProphetNetForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "#BART, BIGBIRD-PEGASUS, BLENDERBOT, LED, FLAN-T5, SWITCH-TRANSFORMER, XLM-PROPHETNET, MVP, PEGASUS, PEGASUS-X, PLBART, PROPHETNET\n",
    "\n",
    "#model_names = [\"facebook/bart-large-cnn\", \"google/bigbird-pegasus-large-arxiv\", \"facebook/blenderbot-400M-distill\", \"allenai/led-base-16384\", \"t5-small\", \"google/switch-base-8\", \"patrickvonplaten/xprophetnet-large-uncased-standalone\", \"RUCAIBox/mvp\", \"google/pegasus-xsum\", \"google/pegasus-x-base\", \"uclanlp/plbart-base\", \"microsoft/prophetnet-large-uncased\"]\n",
    "model_names = [\"t5-small\", \"google/switch-base-8\", \"patrickvonplaten/xprophetnet-large-uncased-standalone\", \"RUCAIBox/mvp\", \"google/pegasus-xsum\", \"google/pegasus-x-base\", \"uclanlp/plbart-base\", \"microsoft/prophetnet-large-uncased\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    \n",
    "    print(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    #model = AutoModelForSeq2SeqLM.from_pretrained(\"model_name\")\n",
    "    \n",
    "    if model_name == \"facebook/bart-large-cnn\":\n",
    "        model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "    elif model_name == \"google/bigbird-pegasus-large-arxiv\":\n",
    "        model = BigBirdPegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "    elif model_name == \"facebook/blenderbot-400M-distill\":\n",
    "        model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "    elif model_name == \"allenai/led-base-16384\":\n",
    "        model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n",
    "        \n",
    "    elif model_name == \"t5-small\":\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "    elif model_name == \"google/switch-base-8\":\n",
    "        model = SwitchTransformersForConditionalGeneration.from_pretrained(\"google/switch-base-8\")\n",
    "\n",
    "    elif model_name == \"patrickvonplaten/xprophetnet-large-uncased-standalone\":\n",
    "        model = XLMProphetNetForConditionalGeneration.from_pretrained(\"patrickvonplaten/xprophetnet-large-uncased-standalone\")\n",
    "\n",
    "    elif model_name == \"RUCAIBox/mvp\":\n",
    "        model = MvpForConditionalGeneration.from_pretrained(\"RUCAIBox/mvp\")\n",
    "\n",
    "    elif model_name == \"google/pegasus-xsum\":\n",
    "        model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
    "    \n",
    "    elif model_name == \"google/pegasus-x-base\":\n",
    "        model = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-base\")\n",
    "\n",
    "    elif model_name == \"uclanlp/plbart-base\":\n",
    "        model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n",
    "\n",
    "    elif model_name == \"microsoft/prophetnet-large-uncased\":\n",
    "        model = ProphetNetForConditionalGeneration.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
    "        \n",
    "    \n",
    "    for index, row in aggregated_aspects.iterrows():\n",
    "        domain = row['Domain']\n",
    "        subdomain = row['Sub Domain']\n",
    "        aspects = row[['Colour', 'Finish', 'Formulation', 'Coverage']]\n",
    "        #print(str(aspects['Aspect1'][0]))\n",
    "        # Prepare input for summarization model (concatenate reviews, aspects, subdomain, and domain)\n",
    "        input_text = f\"Domain: {domain}\\nSub Domain: {subdomain}\\nColour : {str(aspects['Colour'][0])}\\nFinish : {str(aspects['Finish'][0])}\\nFormulation : {str(aspects['Formulation'][0])}\\nCoverage : {str(aspects['Coverage'][0])}\\n{' '.join(map(str, grouped_reviews[(domain, subdomain)]))}\"\n",
    "        #print(input_text)\n",
    "        # Use your summarization model to get the summary\n",
    "        inputs = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        summary_ids = model.generate(inputs, max_length=150, length_penalty=2.0, min_length=50, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        #print(summary)\n",
    "        # Step 6: Save the summaries along with subdomain and domain information\n",
    "        # You can choose to save the summaries in a file, database, or any other preferred format\n",
    "        with open('summary2.txt', 'a') as f:\n",
    "            f.write(f\"Model Name: {model_name}\\n\\nDomain: {domain}\\nSub Domain: {subdomain}\\nSummary: {summary}\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# text = \"The Doctor of Arts (D.A.; occasionally D.Arts or Art.D. from the Latin artium doctor) is a discipline-based terminal doctoral degree that was originally conceived and designed to be an alternative to the traditional research-based Doctor of Philosophy (Ph.D.) and the education-based Doctor of Education (Ed.D.). Like other doctorates, the D.A. is an academic degree of the highest level. The D.A. is also frequently conferred as an honorary degree with the added designation of honoris causa.\"\n",
    "# inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "# summary_ids = model.generate(inputs, max_length=150, length_penalty=2.0, min_length=50, num_beams=4, early_stopping=True)\n",
    "# summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14ad97-fab5-4919-b3fa-b5a975f613b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf703805-69cb-4db8-905f-e359d34bd623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
